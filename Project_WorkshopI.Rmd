---
title: "Project_WorkshopI"
output: html_document
date: "2024-01-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import & Visualize data

```{r}
# Import data
df <- read.csv("insurance.csv")
head(df)
```
```{r}
library(ggplot2)

theme_set(theme_bw( base_size = 15))

ggplot(df, aes(x = age, y = charges, color = charges)) +
  geom_point(show.legend = FALSE) +
  labs(x = "Age", y = "Charges") + 
  scale_color_gradient(low = "blue", high = "red")

ggplot(df, aes(x = bmi, y = charges, color = charges)) +
  geom_point(show.legend = FALSE) +
  labs(x = "Bmi", y = "Charges") + 
  scale_color_gradient(low = "blue", high = "red")
```

```{r}
df_factor <- df
df_factor$children <- factor(df$children)
df_factor$smoker <- factor(df$smoker)
df_factor$region <- factor(df$region)


ggplot(df_factor, aes(x = children, y = charges)) +
  geom_boxplot(position = "dodge", color = "black", aes(fill = children), show.legend = FALSE) +
  labs(x = "Children", y = "Charges") + 
  scale_fill_brewer(palette = "Set3")

ggplot(df_factor, aes(x = smoker, y = charges)) +
  geom_boxplot(position = "dodge", color = "black", aes(fill = smoker), show.legend = FALSE) +
  labs(x = "Smoker", y = "Charges") + 
  scale_fill_brewer(palette = "Set3")

ggplot(df_factor, aes(x = region, y = charges)) +
  geom_boxplot(position = "dodge", color = "black", aes(fill = region), show.legend = FALSE) +
  labs(x = "region", y = "Charges") + 
  scale_fill_brewer(palette = "Set3")

ggplot(df_factor, aes(x = sex, y = charges)) +
  geom_boxplot(position = "dodge", color = "black", aes(fill = sex), show.legend = FALSE) +
  labs(x = "Sex", y = "Charges") + 
  scale_fill_brewer(palette = "Set3")
```

# Gaussian Linear Model

```{r}
# Perform one-hot-encoding to categorical values
encoded_data <- model.matrix(~ . - 1, data = df)
encoded_df <- as.data.frame(encoded_data)

head(encoded_df)
```
```{r}
# Shuffle the dataframe by rows
encoded_df <- encoded_df[sample(nrow(df)), ]

# Split the data into 80% of training and 20% of testing
split_point <- round(nrow(encoded_df) * 0.8)
train_data <- encoded_df[1:split_point,]
test_data <- encoded_df[(split_point + 1):nrow(encoded_df),]
```

```{r}
# Fit Gaussian Linear Model
gl_mdl <- lm(charges ~ ., data = train_data)

summary(gl_mdl)
```

```{r}
library(MASS)

# Detect outliers
stu_residuals <- studres(gl_mdl)

# Create a scatter plot of xi against standardized residuals
par(mfrow = c(1, 1))  # Set up a single plot

indices <- 1:nrow(train_data)

# Create the scatter plot
plot(indices, stu_residuals, xlab = "Indices", ylab = "Studentized Residuals", col = "lightgreen", pch = 16)

# Add a horizontal line at the threshold
threshold <- 2  # Adjust the threshold as needed
abline(h = threshold, col = "red", lty = 2)

# Identify potential outliers (points with standardized residuals greater than a threshold)
outlier_indices <- which(abs(stu_residuals) > 2)  # Adjust the threshold as needed

# Highlight potential outliers on the plot
points(indices[outlier_indices], stu_residuals[outlier_indices], col = "red", pch = 16)
```
```{r}
train_data_neat <- train_data[-outlier_indices,]
gl_mdl_neat <- lm(charges ~ ., data = train_data_neat)

summary(gl_mdl_neat)
```

```{r}
# Analyze standardized residuals
std_residuals <- rstandard(gl_mdl_neat)

# Normal Q-Q Plot
ggplot(mapping = aes(sample = std_residuals)) +
  geom_qq() +
  geom_qq_line(distribution = qnorm, color = "red") +
  labs(title = "Normal Q-Q Plot of Standardized Residuals", x = "Standardized Residuals", y = "Theoratical Quantiles")

# Histogram
ggplot(data.frame(std_residuals = std_residuals), aes(x = std_residuals)) +
  geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(x = "Standardized Residuals", y = "Frequency")
```


```{r}
# Test the model
predictions_glm_neat <- predict(gl_mdl_neat, newdata = test_data)

# Evaluate the model performance by RMSE
library(Metrics)
rmse_glm_neat <- rmse(test_data$charges, predictions_glm_neat)
scaled_rmse_glm_neat <- rmse_glm_neat / mean(test_data$charges)
cat("Root Mean Squared Error (RMSE):", scaled_rmse_glm_neat, "\n")
```
The RMSE is high, suggesting a problem in our model. Maybe the columns are highly correlated.

# Penalized Methods: Lasso

```{r}
library(glmnet)

# Scale the non-categorical columns
train_data_scaled <- train_data
train_data_scaled[,c("age","bmi","children")] <- scale(train_data[,c("age","bmi","children")])

# Use cross validation 
lasso_mdl <- cv.glmnet(as.matrix(train_data_scaled[,-ncol(train_data_scaled)]), train_data_scaled$charges, alpha = 1)

coef_lasso <- coef(lasso_mdl, s = 500)  # lambda.min selects the value that minimizes the cross-validated error
print(coef_lasso)
```

```{r}
# Make predictions on the training data
predictions <- predict(lasso_mdl, as.matrix(train_data_scaled[,-10]))

# Calculate R-squared
mean_response <- mean(train_data$charges)
ss_total <- sum((train_data$charges - mean_response)^2)
ss_residual <- sum((train_data$charges - predictions)^2)
rsquared <- 1 - (ss_residual / ss_total)

# Print the R-squared value
cat("R-squared value:", rsquared, "\n")
```


```{r}
# Test the model
test_data_scaled <- test_data
test_data_scaled[,c("age","bmi","children")] <- scale(test_data[,c("age","bmi","children")])

predictions_lasso <- predict(lasso_mdl, s = 500, newx = as.matrix(test_data_scaled[,-ncol(test_data_scaled)]), type = "response")

# Evaluate the model performance by RMSE
rmse_lasso <- rmse(test_data$charges, predictions_lasso)
scaled_rmse_lasso <- rmse_lasso / mean(test_data$charges)
cat("Root Mean Squared Error (RMSE):", scaled_rmse_lasso, "\n")
```

# Regression Trees

```{r}
# Construct regression tree on all columns
library(rpart)

tree_mdl <- rpart(charges ~ ., data = train_data, method = "anova", control = rpart.control(maxdepth = 10))

# Make predictions on the training data
predictions <- predict(tree_mdl, train_data)

# Calculate R-squared
mean_response <- mean(train_data$charges)
ss_total <- sum((train_data$charges - mean_response)^2)
ss_residual <- sum((train_data$charges - predictions)^2)
rsquared <- 1 - (ss_residual / ss_total)

# Print the R-squared value
cat("R-squared value:", rsquared, "\n")
```

```{r}
# Detect outliers
stu_residuals <- residuals(tree_mdl) / sd(residuals(tree_mdl))

# Create a scatter plot of xi against standardized residuals
par(mfrow = c(1, 1))  # Set up a single plot

# Create a scatter plot of xi against standardized residuals
par(mfrow = c(1, 1))  # Set up a single plot

indices <- 1:nrow(train_data)

# Create the scatter plot
plot(indices, stu_residuals, xlab = "Indices", ylab = "Studentized Residuals", col = "lightgreen", pch = 16)

# Add a horizontal line at the threshold
threshold <- 2  # Adjust the threshold as needed
abline(h = threshold, col = "red", lty = 2)

# Identify potential outliers (points with standardized residuals greater than a threshold)
outlier_indices <- which(abs(stu_residuals) > 2)  # Adjust the threshold as needed

# Highlight potential outliers on the plot
points(indices[outlier_indices], stu_residuals[outlier_indices], col = "red", pch = 16)
```

```{r}
train_data_neat <- train_data[-outlier_indices,]

tree_mdl_neat <- rpart(charges ~ ., data = train_data_neat, method = "anova", control = rpart.control(maxdepth = 10))

# Make predictions on the cleaned training data
predictions <- predict(tree_mdl_neat, train_data_neat)

# Calculate R-squared
mean_response <- mean(train_data_neat$charges)
ss_total <- sum((train_data_neat$charges - mean_response)^2)
ss_residual <- sum((train_data_neat$charges - predictions)^2)
rsquared <- 1 - (ss_residual / ss_total)

# Print the R-squared value
cat("R-squared value:", rsquared, "\n")
```
```{r}
library(rpart.plot)

rpart.plot(tree_mdl_neat)
```


```{r}
# Test the model
predictions_tree <- predict(tree_mdl_neat, test_data)

# Calculate RMSE
rmse_tree <- sqrt(mean((test_data$charges - predictions_tree)^2))

scaled_rmse_tree <- rmse_tree / mean(test_data$charges)
cat("Root Mean Squared Error (RMSE):", scaled_rmse_tree, "\n")
```

```{r}
# Plot true & predicted values in test_data
x <- 1:nrow(test_data)
plot(x, test_data$charges, type = "o", col = "blue", pch = 16, main = "Two Lists in One Plot", xlab = "X-axis", ylab = "Y-axis")

# Overlay the second set of points
points(x, predictions_lasso, type = "o", col = "red", pch = 16)

# Add a legend
legend("topright", legend = c("charges", "predicted_charges"), col = c("blue", "red"), pch = 16, cex = 0.8)
```

# Random Forest

```{r}
library(randomForest)

forest_mdl <- randomForest(charges ~ ., data = train_data, ntree = 100, mtry = 9)
```

```{r}
predictions <- predict(forest_mdl, train_data)

# Calculate R-squared
mean_response <- mean(train_data$charges)
ss_total <- sum((train_data$charges - mean_response)^2)
ss_residual <- sum((train_data$charges - predictions)^2)
rsquared <- 1 - (ss_residual / ss_total)

# Print the R-squared value
cat("R-squared value:", rsquared, "\n")
```

```{r}
# Analyze standardized residuals
residuals <- predict(forest_mdl, train_data) - train_data$charges
std_residuals <- residuals / sd(residuals)

# Normal Q-Q Plot
ggplot(mapping = aes(sample = std_residuals)) +
  geom_qq() +
  geom_qq_line(distribution = qnorm, color = "red") +
  labs(title = "Normal Q-Q Plot of Standardized Residuals", x = "Standardized Residuals", y = "Theoratical Quantiles")

# Histogram
ggplot(data.frame(std_residuals = std_residuals), aes(x = std_residuals)) +
  geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(x = "Standardized Residuals", y = "Frequency")
```

```{r}
# Test the model
predictions_forest <- predict(forest_mdl, test_data)

# Calculate RMSE
rmse_forest <- sqrt(mean((test_data$charges - predictions_forest)^2))

scaled_rmse_forest <- rmse_forest / mean(test_data$charges)
cat("Root Mean Squared Error (RMSE):", scaled_rmse_forest, "\n")
```










